{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import  matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "from tqdm.notebook import trange\n",
    "import os\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = str(datetime.datetime.now())\n",
    "contact_threshold = 0.005\n",
    "RESAMPLE_FACTOR = 24\n",
    "PART_SIZE = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genFiles(idx):\n",
    "    return [ f\"./roughness_dataset/opus/stereo/{idx:02}_{variant}.wav\" for variant in (\"long_light\", \"long_loud\", \"long_medium\", \"short_light\", \"short_loud\", \"short_medium\", \"wiggle\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rough_stones = [0,11,13,15,17,19,1,8,20,22]\n",
    "rough_stones_test = [20,22]\n",
    "                                             \n",
    "for s in rough_stones_test:\n",
    "    rough_stones.remove(s)\n",
    "print(\"rough_training:\",rough_stones)\n",
    "print(\"rough_test:\",rough_stones_test)\n",
    "TRAIN_ROUGH_FILES = [f for i in rough_stones for f in genFiles(i)]\n",
    "rough_wav = torch.cat([torchaudio.load(f)[0][0:2] for f in TRAIN_ROUGH_FILES], dim=1)\n",
    "rough_wav = torchaudio.functional.resample(rough_wav, RESAMPLE_FACTOR, 1)\n",
    "rough_wav.shape, rough_wav.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_stones = [2,3,4,5,6,7,9,10,12,14,16,18,21]\n",
    "smooth_stones_test = [21]\n",
    "\n",
    "for s in smooth_stones_test:\n",
    "    smooth_stones.remove(s)\n",
    "print(\"smooth_training:\",smooth_stones)\n",
    "print(\"smooth_test:\",smooth_stones_test)\n",
    "TRAIN_SMOOTH_FILES = [f for i in smooth_stones for f in genFiles(i)]\n",
    "smooth_wav = torch.cat([torchaudio.load(f)[0][0:2] for f in TRAIN_SMOOTH_FILES], dim=1)\n",
    "smooth_wav = torchaudio.functional.resample(smooth_wav, RESAMPLE_FACTOR, 1)\n",
    "smooth_wav.shape, smooth_wav.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_stepped(x):\n",
    "    ret = []\n",
    "    for i in range(x.shape[1]-PART_SIZE+1):\n",
    "        ret.append(x[:,i:i+PART_SIZE])\n",
    "    ret = torch.stack(ret, dim=0)\n",
    "    return ret\n",
    "\n",
    "rough_parts = cut_stepped(rough_wav)\n",
    "smooth_parts = cut_stepped(smooth_wav)\n",
    "\n",
    "rough_chunks = rough_parts.shape[0]\n",
    "smooth_chunks = smooth_parts.shape[0]\n",
    "imbalance_chunks = rough_chunks-smooth_chunks\n",
    "print(\"rough_chunks:\",rough_chunks,\"smooth_chunks:\",smooth_chunks,\"imbalance_chunks:\",imbalance_chunks)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rough_loudness = rough_parts[:,0].abs().mean(dim=1)\n",
    "rough_loud_parts = rough_parts[rough_loudness > rough_loudness.mean(),:]\n",
    "\n",
    "smooth_loudness = smooth_parts[:,0].abs().mean(dim=1)\n",
    "smooth_loud_parts = smooth_parts[smooth_loudness > smooth_loudness.mean(), :]\n",
    "\n",
    "smooth_silent_parts = smooth_parts[smooth_loudness <= smooth_loudness.mean(),:]\n",
    "\n",
    "rough_loudness_mean = rough_loudness.mean().item()\n",
    "smooth_loudness_mean = smooth_loudness.mean().item()\n",
    "print(\"rough_loudness:\",rough_loudness_mean,\"smooth_loudness:\",smooth_loudness_mean)\n",
    "\n",
    "rough_loud_parts_num = rough_loud_parts.shape[0]\n",
    "smooth_loud_parts_num = smooth_loud_parts.shape[0]\n",
    "smooth_silent_parts_num = smooth_silent_parts.shape[0]\n",
    "print(\"rough_loud_parts_num:\",rough_loud_parts_num,\"smooth_loud_parts_num:\",smooth_loud_parts_num,\"smooth_silent_parts_num:\",smooth_silent_parts_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        part_size = PART_SIZE\n",
    "        \n",
    "        self.resampler = torchaudio.transforms.Resample(RESAMPLE_FACTOR, 1, dtype=torch.float32)\n",
    "        #self.layer1 = torch.nn.Linear((part_size//2+1), 1024) # one microphone\n",
    "        self.layer1 = torch.nn.Linear((part_size//2+1)+(part_size//2+1), 1024) # two microphone\n",
    "        self.layer2 = torch.nn.Linear(1024, 512)\n",
    "        self.layer3 = torch.nn.Linear(512, 256)\n",
    "        \n",
    "        self.layer_resnet = torch.nn.ModuleList([torch.nn.Linear(256, 256) for i in range(10)])\n",
    "        \n",
    "        self.layer4 = torch.nn.Linear(256, 128)\n",
    "        self.layer5 = torch.nn.Linear(128, 64)\n",
    "        self.linear = torch.nn.Linear(64, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.shape[-1] != 512:\n",
    "            x = self.resampler(x)\n",
    "        \n",
    "        fft_data = torch.fft.rfft(x[:,0])\n",
    "        fft_as_real = torch.view_as_real(fft_data)\n",
    "        fft = fft_as_real.norm(p=2, dim=-1)\n",
    "        \n",
    "        air_fft_data = torch.fft.rfft(x[:,1])\n",
    "        air_fft_as_real = torch.view_as_real(air_fft_data)\n",
    "        air_fft = air_fft_as_real.norm(p=2, dim=-1)\n",
    "        \n",
    "        #x = torch.nn.functional.relu(self.layer1(fft)) # one microphone\n",
    "        x = torch.nn.functional.relu(self.layer1(torch.cat([fft, air_fft], dim=1))) # two microphone\n",
    "        x = torch.nn.functional.relu(self.layer2(x))\n",
    "        x = torch.nn.functional.relu(self.layer3(x))\n",
    "        \n",
    "        for layer in self.layer_resnet:\n",
    "            x = x + torch.nn.functional.relu(layer(x))\n",
    "\n",
    "        x = torch.nn.functional.relu(self.layer4(x))\n",
    "        x = torch.nn.functional.relu(self.layer5(x))\n",
    "        x = self.linear(x)\n",
    "        x = torch.nn.functional.log_softmax(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "model = Model().cuda()\n",
    "data = torch.cat([smooth_silent_parts, smooth_loud_parts, rough_loud_parts], dim=0)\n",
    "labels = torch.cat([torch.zeros(smooth_silent_parts.shape[0], dtype=torch.long),\n",
    "    torch.ones(smooth_loud_parts.shape[0], dtype=torch.long),\n",
    "    2*torch.ones(rough_loud_parts.shape[0], dtype=torch.long)], dim=0)\n",
    "class_weights = torch.tensor([1.0, 0.05, 0.05]).cuda()\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(data, labels)\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    batch_size=6000,\n",
    "    num_workers=6\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "all_losses = []\n",
    "\n",
    "for epoch in trange(5):\n",
    "    for batch_data, batch_label in loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_data = batch_data.cuda()\n",
    "        batch_data = batch_data + 0.005 * torch.randn_like(batch_data)\n",
    "        \n",
    "        loss = torch.nn.functional.nll_loss(model(batch_data.cuda()), batch_label.cuda(), weight=class_weights)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "end = time.time()\n",
    "training_time = end-start\n",
    "print(\"Trained for\",training_time,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(identifier)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'RESAMPLE_FACTOR': RESAMPLE_FACTOR,\n",
    "    'PART_SIZE': PART_SIZE,\n",
    "    'contact_threshold': contact_threshold,\n",
    "    'rough_stones': rough_stones,\n",
    "    'rough_stones_test': rough_stones_test,\n",
    "    'smooth_stones': smooth_stones,\n",
    "    'smooth_stones_test': smooth_stones_test,\n",
    "    'rough_chunks': rough_chunks,\n",
    "    'smooth_chunks': smooth_chunks,\n",
    "    'imbalance_chunks': imbalance_chunks,\n",
    "    'training_time': training_time,\n",
    "    'rough_loudness_mean': rough_loudness_mean,\n",
    "    'smooth_loudness_mean': smooth_loudness_mean,\n",
    "}, \"./\"+identifier+\"/model.pt\")\n",
    "\n",
    "scriptModel = torch.jit.script(model.cpu())\n",
    "scriptModel.save(\"./\"+identifier+\"/model_script.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
